{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies: \n",
    "!pip install pyyaml==5.1\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "# opencv is pre-installed on colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc68ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# img_file = \"Geel/Geel-002.jpg\"\n",
    "# img_file = \"Geel/MarktGeel-3.jpg\"\n",
    "img_file = \"Geel/2016 - Kristof DonneÃÅ - Spelende kinderen Markt-2.jpg\"\n",
    "img_filename, img_file_extension = os.path.splitext(img_file)\n",
    "\n",
    "im = cv2.imread(img_file)\n",
    "\n",
    "# !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "# im = cv2.imread(\"./input.jpg\")\n",
    "# im = cv2.imread(\"./juli-kosolapova-oGvDWyYct4Y-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./eliott-reyna-jCEpN62oWL4-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./kin-li-UOYkntGPvNE-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./gabriella-clare-marino-2vgx9eOmH0I-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./gabriella-clare-marino-ms6tf_QVeSQ-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./jan-antonin-kolar-LuBJ1GSvBl4-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./alex-vasey-9UhOKfAtjXA-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./alex-vasey-oLjUXxYX8lQ-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./2009-Italie-hires-148.jpg\")\n",
    "# im = cv2.imread(\"./orlando-allo-qRpOzXWsu3c-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./allen-taylor-0cmJRW5rOJ8-unsplash.jpg\")\n",
    "# im = cv2.imread(\"./antonio-sessa-8TWBBmha7gI-unsplash.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11345",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (200,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34893b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4be2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(outputs[\"instances\"].pred_classes.size())\n",
    "# print(outputs[\"instances\"].pred_boxes)\n",
    "# print(outputs[\"instances\"].pred_classes)\n",
    "# print(outputs[\"instances\"].pred_masks.size())\n",
    "# print(outputs[\"instances\"])\n",
    "# print(vars(outputs[\"instances\"])['_fields'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im.size\n",
    "im.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import GenericMask\n",
    "\n",
    "img = im.copy()\n",
    "\n",
    "img_zero = np.ones(img.shape,dtype=np.uint8)\n",
    "img_zero[:,:] = (24,10,10)\n",
    "alpha_mask = np.ones(img.shape,dtype=np.uint8)*255\n",
    "\n",
    "for i in range(outputs[\"instances\"].pred_classes.shape[0]) :\n",
    "    instance_class = outputs[\"instances\"].pred_classes[i]\n",
    "    if(instance_class == 0) :\n",
    "        polymasks = GenericMask(outputs[\"instances\"].pred_masks[i].numpy(), img.shape[0], img.shape[1]).polygons\n",
    "        for polymask in polymasks :\n",
    "            polymask_pairs = [ [x,y] for x,y in zip(polymask[0::2], polymask[1::2]) ]\n",
    "            pts = np.array([polymask_pairs], np.int32)\n",
    "            # directly on the image, no blurring: \n",
    "            # cv2.fillPoly(img, pts, color=(0, 0, 0))\n",
    "            # cv2.polylines(img, pts, isClosed=True, color=(255,255,255), thickness=1, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            cv2.polylines(img_zero, pts, isClosed=True, color=(255,255,255), thickness=1, lineType=cv2.LINE_AA)\n",
    "            cv2.fillPoly(alpha_mask, pts, color=(0, 0, 0))\n",
    "\n",
    "img_zero_line = img_zero.copy()\n",
    "img_blur = cv2.GaussianBlur(img_zero, (9,9), 11)\n",
    "img_blur2 = cv2.GaussianBlur(img_zero, (5,5), 7)\n",
    "img_people = cv2.add(img_blur, img_blur2)\n",
    "img_people = cv2.add(img_people, img_zero_line)\n",
    "\n",
    "# https://learnopencv.com/alpha-blending-using-opencv-cpp-python/\n",
    "background = img.astype(float)\n",
    "# background = np.zeros(img.shape,dtype=np.uint8).astype(float)\n",
    "people = img_people.astype(float)\n",
    "alpha = alpha_mask.astype(float)/255\n",
    "\n",
    "people = cv2.multiply(1.0-alpha, people)\n",
    "background = cv2.multiply(alpha, background)\n",
    "overlay = np.array(cv2.add(people, background), dtype=np.uint8)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (200,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(img_filename + \"-seg\" + img_file_extension, overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = im[:, :, ::-1]\n",
    "v = Visualizer(img, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "img_block = out.get_image()[:, :, ::-1]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_block, cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (200,100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(img_filename + \"-block\" + img_file_extension, img_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd74b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with a keypoint detection model\n",
    "cfg = get_cfg()   # get a fresh new config\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a37517",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_key = overlay.copy()\n",
    "\n",
    "if(outputs[\"instances\"].has(\"pred_boxes\")) :\n",
    "    outputs[\"instances\"].remove(\"pred_boxes\") # don't draw the boxes\n",
    "\n",
    "v = Visualizer(im_key[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "overlay_skel = out.get_image()[:, :, ::-1]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(overlay_skel, cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (300,150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(img_filename + \"-skel\" + img_file_extension, overlay_skel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e22964",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_key = im.copy()\n",
    "v = Visualizer(im_key[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bae22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (300,150)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ab3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------DEKR----------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ab456",
   "metadata": {},
   "outputs": [],
   "source": [
    "dekr_folder=\"/src/DEKR\"\n",
    "import sys\n",
    "sys.path.append(dekr_folder+\"/lib\")\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e609e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(dekr_folder+\"/tools\")\n",
    "import _init_paths\n",
    "import models\n",
    "\n",
    "from config import cfg as cfg_dekr\n",
    "from config import update_config\n",
    "from core.inference import get_multi_stage_outputs\n",
    "from core.inference import aggregate_results\n",
    "from core.nms import pose_nms\n",
    "from core.match import match_pose_to_heatmap\n",
    "from utils.transforms import resize_align_multi_scale\n",
    "from utils.transforms import get_final_preds\n",
    "from utils.transforms import get_multi_scale_size\n",
    "from utils.transforms import up_interpolate\n",
    "\n",
    "CTX = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c42c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_KEYPOINT_INDEXES = {\n",
    "    0: 'nose',\n",
    "    1: 'left_eye',\n",
    "    2: 'right_eye',\n",
    "    3: 'left_ear',\n",
    "    4: 'right_ear',\n",
    "    5: 'left_shoulder',\n",
    "    6: 'right_shoulder',\n",
    "    7: 'left_elbow',\n",
    "    8: 'right_elbow',\n",
    "    9: 'left_wrist',\n",
    "    10: 'right_wrist',\n",
    "    11: 'left_hip',\n",
    "    12: 'right_hip',\n",
    "    13: 'left_knee',\n",
    "    14: 'right_knee',\n",
    "    15: 'left_ankle',\n",
    "    16: 'right_ankle'\n",
    "}\n",
    "\n",
    "\n",
    "CROWDPOSE_KEYPOINT_INDEXES = {\n",
    "    0: 'left_shoulder',\n",
    "    1: 'right_shoulder',\n",
    "    2: 'left_elbow',\n",
    "    3: 'right_elbow',\n",
    "    4: 'left_wrist',\n",
    "    5: 'right_wrist',\n",
    "    6: 'left_hip',\n",
    "    7: 'right_hip',\n",
    "    8: 'left_knee',\n",
    "    9: 'right_knee',\n",
    "    10: 'left_ankle',\n",
    "    11: 'right_ankle',\n",
    "    12: 'head',\n",
    "    13: 'neck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcf1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_estimation_prediction(cfg, model, image, vis_thre, transforms):\n",
    "    # size at scale 1.0\n",
    "    base_size, center, scale = get_multi_scale_size(\n",
    "        image, cfg.DATASET.INPUT_SIZE, 1.0, 1.0\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        heatmap_sum = 0\n",
    "        poses = []\n",
    "\n",
    "        for scale in sorted(cfg.TEST.SCALE_FACTOR, reverse=True):\n",
    "            image_resized, center, scale_resized = resize_align_multi_scale(\n",
    "                image, cfg.DATASET.INPUT_SIZE, scale, 1.0\n",
    "            )\n",
    "\n",
    "            image_resized = transforms(image_resized)\n",
    "            image_resized = image_resized.unsqueeze(0)\n",
    "            # image_resized = image_resized.unsqueeze(0).cuda()\n",
    "\n",
    "            heatmap, posemap = get_multi_stage_outputs(\n",
    "                cfg, model, image_resized, cfg.TEST.FLIP_TEST\n",
    "            )\n",
    "            heatmap_sum, poses = aggregate_results(\n",
    "                cfg, heatmap_sum, poses, heatmap, posemap, scale\n",
    "            )\n",
    "        \n",
    "        heatmap_avg = heatmap_sum/len(cfg.TEST.SCALE_FACTOR)\n",
    "        poses, scores = pose_nms(cfg, heatmap_avg, poses)\n",
    "\n",
    "        if len(scores) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            if cfg.TEST.MATCH_HMP:\n",
    "                poses = match_pose_to_heatmap(cfg, poses, heatmap_avg)\n",
    "\n",
    "            final_poses = get_final_preds(\n",
    "                poses, center, scale_resized, base_size\n",
    "            )\n",
    "\n",
    "        final_results = []\n",
    "        for i in range(len(scores)):\n",
    "            if scores[i] > vis_thre:\n",
    "                final_results.append(final_poses[i])\n",
    "\n",
    "        if len(final_results) == 0:\n",
    "            return []\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types # for SimpleNamespace\n",
    "\n",
    "# transformation\n",
    "pose_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# cudnn related setting\n",
    "cudnn.benchmark = cfg_dekr.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg_dekr.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg_dekr.CUDNN.ENABLED\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.cfg = dekr_folder+\"/experiments/coco/inference_demo_coco.yaml\"\n",
    "args.opts = [\"TEST.MODEL_FILE\", dekr_folder+\"/model/pose_coco/pose_dekr_hrnetw32_coco.pth\"]\n",
    "args.visthre = '0.3'\n",
    "# print(args)\n",
    "update_config(cfg_dekr, args)\n",
    "\n",
    "\n",
    "# print(cfg_dekr.MODEL.NAME)\n",
    "# print(cfg_dekr.MODEL.SPEC)\n",
    "pose_model = models.hrnet_dekr.get_pose_net(cfg_dekr, is_train=False)\n",
    "\n",
    "pose_model = eval('models.'+cfg_dekr.MODEL.NAME+'.get_pose_net')(\n",
    "        cfg_dekr, is_train=False\n",
    ")\n",
    "\n",
    "if cfg_dekr.TEST.MODEL_FILE:\n",
    "    print('=> loading model from {}'.format(cfg_dekr.TEST.MODEL_FILE))\n",
    "    pose_model.load_state_dict(torch.load(\n",
    "        cfg_dekr.TEST.MODEL_FILE, map_location=CTX), strict=False)\n",
    "else:\n",
    "    raise ValueError('expected model defined in config at TEST.MODEL_FILE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681195cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model.to(CTX)\n",
    "pose_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "image_pose = image_rgb.copy()\n",
    "\n",
    "# The code below doesn't seem to run without GPU. There is a .cuda() call deeper in the DEKR core library.\n",
    "# The writers of the code clearly didn't test the CPU version :-) \n",
    "pose_preds = get_pose_estimation_prediction(cfg_dekr, pose_model, image_pose, args.visthre, transforms=pose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7577bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 /src/DEKR/tools/inference_demo.py\n",
    "\n",
    "!python tools/inference_demo.py --cfg experiments/coco/inference_demo_coco.yaml \\\n",
    "    --videoFile ../multi_people.mp4 \\\n",
    "    --outputDir output \\\n",
    "    --visthre 0.3 \\\n",
    "    TEST.MODEL_FILE model/pose_coco/pose_dekr_hrnetw32.pth\n",
    "!python tools/inference_demo.py --cfg experiments/crowdpose/inference_demo_crowdpose.yaml \\\n",
    "    --videoFile ../multi_people.mp4 \\\n",
    "    --outputDir output \\\n",
    "    --visthre 0.3 \\\n",
    "    TEST.MODEL_FILE model/pose_crowdpose/pose_dekr_hrnetw32.pth \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495f0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
